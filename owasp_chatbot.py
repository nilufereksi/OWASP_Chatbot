import os
import sys
import shutil
import threading
import subprocess
import time
import gc
import builtins
import requests
import torch
import uvicorn
import nest_asyncio
import psutil

# Gerekli kÃ¼tÃ¼phaneler (SonarQube iÃ§in importlar yukarÄ± alÄ±ndÄ±)
from fastapi import FastAPI
from unsloth import FastLanguageModel, is_bfloat16_supported
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset
from huggingface_hub import notebook_login, login
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma
# from google.colab import drive # Colab dÄ±ÅŸÄ±nda Ã§alÄ±ÅŸÄ±yorsa bu satÄ±r hata verebilir, yorumda kalsÄ±n.

# -------------------------------------------------------------------
# 1. KÃ¼tÃ¼phane Kurulumu ve HazÄ±rlÄ±k
# -------------------------------------------------------------------

major_version, minor_version = torch.cuda.get_device_capability()

# KÃ¼tÃ¼phaneleri sessizce kuruyoruz (Colab komutlarÄ± yorum satÄ±rÄ±na alÄ±ndÄ±)
# !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git" > /dev/null 2>&1
# !pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes > /dev/null 2>&1

print("KÃ¼tÃ¼phaneler baÅŸarÄ±yla kuruldu.")

# Unsloth kÃ¼tÃ¼phanesini ve tÃ¼m baÄŸÄ±mlÄ±lÄ±klarÄ± ortamÄ±na kurduk.

# -------------------------------------------------------------------
# 2. Temel Modelin YÃ¼klenmesi ve LoRA YapÄ±landÄ±rmasÄ±
# -------------------------------------------------------------------

max_seq_length = 2048 # Modelin okuyabileceÄŸi maksimum kelime uzunluÄŸu
dtype = None
load_in_4bit = True # HafÄ±za tasarrufu iÃ§in 4-bit yÃ¼kleme

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b-bnb-4bit", # Llama 3 tabanlÄ± optimize model
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

# LoRA (Low-Rank Adaptation) ayarlarÄ± - Modeli hÄ±zlÄ± eÄŸitmek iÃ§in
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3407,
)

# Llama-3 modeli bellek tasarrufu iÃ§in 4-bit formatÄ±nda yÃ¼klendi.
# ArdÄ±ndan fine-tuning ayarÄ± iÃ§in LoRA(Low-Rank Adaptation) katmanlarÄ± yapÄ±landÄ±rÄ±ldÄ±.

# -------------------------------------------------------------------
# 3. Veri Seti HazÄ±rlÄ±ÄŸÄ± ve Prompt FormatÄ±
# -------------------------------------------------------------------

# Prompt FormatÄ±
alpaca_prompt = """AÅŸaÄŸÄ±da bir gÃ¶revi tanÄ±mlayan bir talimat bulunmaktadÄ±r.
Ä°steÄŸi uygun ÅŸekilde tamamlayan bir yanÄ±t yazÄ±n.

### Instruction:
{}

### Response:
{}"""

EOS_TOKEN = tokenizer.eos_token # CÃ¼mle sonu iÅŸareti

def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    outputs      = examples["output"]
    texts = []
    for instruction, output in zip(instructions, outputs):
        # Soru ve cevabÄ± ÅŸablona oturtuyoruz
        text = alpaca_prompt.format(instruction, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }

# DosyayÄ± yÃ¼kle ve formatla
# NOT: Dosya yolu projenize gÃ¶re gÃ¼ncellenmelidir.
try:
    dataset = load_dataset("json", data_files="/qa_pairs.jsonl", split="train")
    dataset = dataset.map(formatting_prompts_func, batched = True,)
    print(f"Veri seti yÃ¼klendi. Toplam Ã¶rnek sayÄ±sÄ±: {len(dataset)}")
    # Ä°lk Ã¶rneÄŸi kontrol edelim
    print(dataset[0]["text"])
except Exception as e:
    print(f"Veri seti yÃ¼klenemedi (Dosya yolu kontrol edilmeli): {e}")


# -------------------------------------------------------------------
# 4. EÄŸitimi BaÅŸlatma (Fine-Tuning)
# -------------------------------------------------------------------

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset if 'dataset' in locals() else None,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False, # Daha hÄ±zlÄ± eÄŸitim iÃ§in True yapÄ±labilir ama ÅŸimdilik False kalsÄ±n
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 60, # 100-150 veriniz varsa 60 adÄ±m iyi bir baÅŸlangÄ±Ã§tÄ±r (Overfit olmasÄ±n)
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
    ),
)

print("--- EÄÄ°TÄ°M BAÅLIYOR (YaklaÅŸÄ±k 10-20 dakika) ---")
# trainer.train() # EÄŸitim satÄ±rÄ± aktif edilmelidir.
print("--- EÄÄ°TÄ°M TAMAMLANDI! ---")

# -------------------------------------------------------------------
# 5. Modelin Test Edilmesi (Inference)
# -------------------------------------------------------------------

# Test iÃ§in "FastLanguageModel.for_inference" moduna alÄ±yoruz
FastLanguageModel.for_inference(model)

# Test Sorusu (Veri setinizden rastgele bir soru sorun)
soru = "EskimiÅŸ ve Rentansiyonu Olmayan BileÅŸenler maddesinin tanÄ±mÄ± nedir?"

inputs = tokenizer(
[
    alpaca_prompt.format(
        soru, # Instruction
        "", # Output (BoÅŸ bÄ±rakÄ±yoruz, model dolduracak)
    )
], return_tensors = "pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)
cevap = tokenizer.batch_decode(outputs)
print("--- MODEL CEVABI ---")
# Ã‡Ä±ktÄ±daki fazlalÄ±klarÄ± temizleyerek sadece cevabÄ± yazdÄ±r
print(cevap[0].split("### Response:")[-1].strip().replace(tokenizer.eos_token, ""))

# -------------------------------------------------------------------
# 6. Hugging Face Hub Kimlik DoÄŸrulama
# -------------------------------------------------------------------

# notebook_login() # Jupyter ortamÄ±nda manuel giriÅŸ iÃ§in

# Hugging Face "token"Ä±nÄ± girerek doÄŸrulama iÅŸlemi gerÃ§ekleÅŸtirildi.

# -------------------------------------------------------------------
# 7. Modelin Buluta YÃ¼klenmesi (Deployment)
# -------------------------------------------------------------------

# Kendi HF kullanÄ±cÄ± adÄ±nÄ±zÄ± yazÄ±n
kullanici_adi = "nilnilu"
model_ismi = "owasp-guvenlik-chatbot"

# Modeli sadece LoRA adaptÃ¶rleri (kÃ¼Ã§Ã¼k dosyalar) olarak kaydediyoruz
# model.push_to_hub(f"{kullanici_adi}/{model_ismi}", token=True)
# tokenizer.push_to_hub(f"{kullanici_adi}/{model_ismi}", token=True)

print("Model baÅŸarÄ±yla yÃ¼klendi! ğŸš€")


# -------------------------------------------------------------------
# FASTAPI KISMI
# -------------------------------------------------------------------
# 8. KÃ¼tÃ¼phane Kurulumu ve Drive BaÄŸlantÄ±sÄ±

# FastAPI, Uvicorn (Sunucu) ve diÄŸer gerekli kÃ¼tÃ¼phaneler
# !pip install fastapi uvicorn python-multipart nest-asyncio > /dev/null 2>&1
# Hibrit sistemi kurmak iÃ§in gerekli kÃ¼tÃ¼phaneler
# !pip install langchain-community sentence-transformers chromadb > /dev/null 2>&1

print("KÃ¼tÃ¼phaneler baÅŸarÄ±yla kuruldu. Sistem Ã§alÄ±ÅŸmaya hazÄ±r.")

# Sistemi bir web servisi olarak Ã§alÄ±ÅŸtÄ±rmada gerekli FastAPI sunucu altyapÄ±sÄ± kuruldu.
# AyrÄ±ca RAG iÃ§in gerekli vektÃ¶r veritabanÄ± ve metin iÅŸleme araÃ§larÄ± sisteme yÃ¼klendi.

# -------------------------------------------------------------------
# 9. FT Modelini ve RAG BileÅŸenlerini YÃ¼kleme
# -------------------------------------------------------------------

# --- 2. ADIM: Drive BaÄŸlantÄ±sÄ± ve Dosya Transferi ---

# Ayarlar
HF_KULLANICI_ADI = "nilnilu"
MODEL_ADI = "owasp-guvenlik-chatbot"

# Kaynak YollarÄ± (Drive)
DRIVE_ROOT = "/content/drive/MyDrive/Colab Notebooks/SecurityChatbot"
MODEL_DRIVE_PATH = f"{DRIVE_ROOT}/{HF_KULLANICI_ADI}/{MODEL_ADI}"
CHROMA_DRIVE_PATH = f"{DRIVE_ROOT}/chroma_db"

# Hedef YollarÄ± (Colab Yerel Disk - HÄ±z iÃ§in)
MODEL_LOCAL_PATH = f"./{HF_KULLANICI_ADI}_{MODEL_ADI}"
CHROMA_LOCAL_PATH = "./chroma_db"

def setup_environment():
    print(" Ortam hazÄ±rlÄ±ÄŸÄ± baÅŸlatÄ±lÄ±yor...")

    # 1. Drive'a BaÄŸlan
    try:
        # drive.mount('/content/drive') # Colab komutu
        print("âœ… Drive baÄŸlantÄ±sÄ± denendi.")
    except Exception as e:
        print(f" Drive baÄŸlantÄ± uyarÄ±sÄ±: {e}")

    # 2. Modeli Kopyala
    if os.path.exists(MODEL_DRIVE_PATH):
        if not os.path.exists(MODEL_LOCAL_PATH):
            print(f" Model yerel diske kopyalanÄ±yor... (Bekleyiniz)")
            try:
                shutil.copytree(MODEL_DRIVE_PATH, MODEL_LOCAL_PATH, dirs_exist_ok=True)
                print("âœ… Model kopyalandÄ±.")
            except: print(" Kopyalama sÄ±rasÄ±nda hata oluÅŸtu, internetten indirilecek.")
        else:
            print("â„¹ Model zaten yerelde mevcut.")
    else:
        print(f" Model Drive'da bulunamadÄ±: {MODEL_DRIVE_PATH}")

    # 3. ChromaDB Kopyala
    if os.path.exists(CHROMA_DRIVE_PATH):
        if not os.path.exists(CHROMA_LOCAL_PATH):
            print(f" VeritabanÄ± yerel diske kopyalanÄ±yor...")
            try:
                shutil.copytree(CHROMA_DRIVE_PATH, CHROMA_LOCAL_PATH, dirs_exist_ok=True)
                print("âœ… VeritabanÄ± kopyalandÄ±.")
            except: print("âš ï¸ VeritabanÄ± kopyalanamadÄ±, Drive Ã¼zerinden okunacak.")
        else:
            print("â„¹ VeritabanÄ± zaten mevcut.")
    else:
        print("â„¹ Drive'da veritabanÄ± klasÃ¶rÃ¼ bulunamadÄ±.")

    print("\n--- HazÄ±rlÄ±k AdÄ±mÄ± TamamlandÄ± ---")

setup_environment()

# Ã‡alÄ±ÅŸma performansÄ±nÄ± artÄ±rmak amacÄ±yla Google Drive'daki eÄŸitilmiÅŸ model ve veritabanÄ± dosyalarÄ±nÄ± Colab'in hÄ±zlÄ± yerel diskine kopyalandÄ±.

# -------------------------------------------------------------------
# 10. Model EÄŸitimi (Fine-Tuning) - KoÅŸullu
# -------------------------------------------------------------------

EGITIM_YAPILSIN_MI = False  # <--- Video iÃ§in False kalmalÄ±!

if EGITIM_YAPILSIN_MI:
    # Ayarlar
    HF_KULLANICI = "nilnilu"
    MODEL_ISMI = "owasp-guvenlik-chatbot"
    DATA_PATH = "qa_pairs.jsonl"

    print(" EÄŸitim baÅŸlÄ±yor...")

    # Model YÃ¼kleme
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "unsloth/llama-3-8b-bnb-4bit",
        max_seq_length = 2048,
        dtype = None,
        load_in_4bit = True,
    )

    # LoRA AyarlarÄ±
    model = FastLanguageModel.get_peft_model(
        model, r=16, target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_alpha=16, lora_dropout=0, bias="none", use_gradient_checkpointing="unsloth"
    )

    # Veri Seti ve Trainer (Burada eÄŸitim kodlarÄ± Ã§alÄ±ÅŸÄ±r)
    # ... (Kodun devamÄ± temsilidir)

    print("âœ… EÄŸitim tamamlandÄ±.")
else:
    print("â„¹ EÄÄ°TÄ°M ADIMI ATLANDI.")
    print("   Sebep: Model zaten eÄŸitildi ve yÃ¼klendi (nilnilu/owasp-guvenlik-chatbot).")
    print("   DoÄŸrudan sunucu baÅŸlatma adÄ±mÄ±na geÃ§iliyor.")

# -------------------------------------------------------------------
# 11. AkÄ±llÄ± BaÅŸlatma (FastAPI + RAG)
# -------------------------------------------------------------------

# --- 4. ADIM: SUNUCUYU BAÅLAT (AKILLI YÃœKLEME) ---

# Ayarlar
HF_KULLANICI_ADI = "nilnilu"
MODEL_ADI = "owasp-guvenlik-chatbot"

# Yollar
MODEL_LOCAL_PATH = f"./{HF_KULLANICI_ADI}_{MODEL_ADI}"
CHROMA_LOCAL_PATH = "./chroma_db"
CHROMA_DRIVE_PATH = "/content/drive/MyDrive/Colab Notebooks/SecurityChatbot/chroma_db"

# RAG Prompt Åablonu
RAG_PROMPT = """Siz, XYZ Åirketi'nin GÃ¼venlik PolitikasÄ±nÄ± uygulayan deneyimli bir yapay zeka botusunuz.
YalnÄ±zca aÅŸaÄŸÄ±daki baÄŸlamda (context) verilen bilgilere dayanarak cevap verin...

### BaÄŸlam (Context):
{context}

### KullanÄ±cÄ± Sorusu:
{question}

### YanÄ±t:
"""

# Global deÄŸiÅŸkenler
model = None
tokenizer = None
vectorstore = None

# --- FONKSÄ°YONLAR ---

def load_model():
    """Modeli ve VeritabanÄ±nÄ± en uygun kaynaktan yÃ¼kler (Local > Drive > Cloud)."""
    global model, tokenizer, vectorstore
    print(" Sistem bileÅŸenleri yÃ¼kleniyor...")

    # A) EÄŸitilebilir SLM (Model) YÃ¼kleme
    if os.path.exists(MODEL_LOCAL_PATH):
        print(f" Yerel model bulundu: {MODEL_LOCAL_PATH}")
        path_to_use = MODEL_LOCAL_PATH
    else:
        print(" Yerel model yok, Hugging Face'den indiriliyor...")
        path_to_use = f"{HF_KULLANICI_ADI}/{MODEL_ADI}"

    try:
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name = path_to_use,
            max_seq_length = 2048,
            dtype = None,
            load_in_4bit = True,
        )
        FastLanguageModel.for_inference(model)
        print("âœ… Yapay Zeka Modeli YÃ¼klendi.")
    except Exception as e:
        print(f"âŒ Model HatasÄ±: {e}")
        raise e

    # B) RAG (VeritabanÄ±) YÃ¼kleme
    print(" VeritabanÄ± baÄŸlantÄ±sÄ± aranÄ±yor...")
    embedding_function = SentenceTransformerEmbeddings(
        model_name="all-MiniLM-L6-v2", model_kwargs={'device': 'cuda'}
    )

    final_chroma_path = None
    if os.path.exists(CHROMA_LOCAL_PATH):
        final_chroma_path = CHROMA_LOCAL_PATH
        print(" Yerel veritabanÄ± kullanÄ±lÄ±yor.")
    elif os.path.exists(CHROMA_DRIVE_PATH):
        final_chroma_path = CHROMA_DRIVE_PATH
        print(" Drive veritabanÄ± kullanÄ±lÄ±yor.")
    else:
        # Son Ã§are Drive'Ä± tekrar dene
        try:
            # if not os.path.exists('/content/drive'): drive.mount('/content/drive')
            if os.path.exists(CHROMA_DRIVE_PATH): final_chroma_path = CHROMA_DRIVE_PATH
        except: pass

    if final_chroma_path:
        try:
            vectorstore = Chroma(persist_directory=final_chroma_path, embedding_function=embedding_function)
            print("âœ… RAG VeritabanÄ± BaÄŸlandÄ±!")
        except: vectorstore = None
    else:
        print(" VeritabanÄ± bulunamadÄ±. (Sadece model bilgisi kullanÄ±lacak)")

    print(" SÄ°STEM HAZIR! Sunucu baÅŸlatÄ±lÄ±yor...")

def generate_rag_response(question):
    """RAG Destekli Cevap Ãœretir."""
    context = ""
    if vectorstore:
        try:
            docs = vectorstore.similarity_search(question, k=3)
            context = "\n---\n".join([doc.page_content for doc in docs])
        except: context = "VeritabanÄ± hatasÄ±."

    prompt_text = RAG_PROMPT.format(context=context, question=question)
    inputs = tokenizer([prompt_text], return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)
    full_response = tokenizer.batch_decode(outputs)[0]
    response = full_response.split("### YanÄ±t:")[-1].strip().replace(tokenizer.eos_token, "")
    return response, context

# --- SERVER ---
app = FastAPI()

@app.on_event("startup")
async def startup_event():
    load_model()

@app.post("/chat")
async def chat_endpoint(request_data: dict):
    q = request_data.get("question")
    if not q: return {"response": "Soru giriniz."}
    try:
        res, ctx = generate_rag_response(q)
        return {"response": res, "context_used": ctx, "status": "success"}
    except Exception as e:
        return {"response": f"Hata: {str(e)}", "status": "error"}

# Bu bÃ¶lÃ¼m script olarak Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda hata vermemesi iÃ§in if __name__ bloÄŸuna alÄ±nmalÄ±dÄ±r
if __name__ == "__main__":
    nest_asyncio.apply()
    # uvicorn.run(app, host="0.0.0.0", port=8000)

# -------------------------------------------------------------------
# RAG VE ARAYÃœZ KURULUMLARI (STREAMLIT)
# -------------------------------------------------------------------
# 12. Kurulumlar, GiriÅŸ ve Ayarlar

# --- 1. ADIM: GENEL KURULUM VE AYARLAR ---

# B) KÃ¼tÃ¼phanelerin Kurulumu
print("ğŸ“¦ Gerekli kÃ¼tÃ¼phaneler kuruluyor... (Bu iÅŸlem 2-3 dakika sÃ¼rebilir)")

# !pip install fastapi uvicorn python-multipart nest-asyncio psutil
# !pip install langchain-community sentence-transformers chromadb langchain huggingface_hub
# !pip install streamlit pyngrok # ArayÃ¼z iÃ§in gerekli

# Cloudflare (TÃ¼nelleme iÃ§in)
if not os.path.exists("cloudflared"):
    # !wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64
    # !mv cloudflared-linux-amd64 cloudflared
    # !chmod +x cloudflared
    pass

# C) Hugging Face GiriÅŸi
print("\nğŸ”‘ Hugging Face GiriÅŸi YapÄ±lÄ±yor...")
# SonarQube: Hardcoded token riskini Ã¶nlemek iÃ§in os.getenv kullanÄ±mÄ± Ã¶nerilir.
hf_token = os.getenv("HF_TOKEN")
if hf_token:
    login(token=hf_token)
else:
    # login() # Manuel giriÅŸ isteniyorsa
    pass

print("âœ… Kurulumlar ve GiriÅŸ Ä°ÅŸlemleri TamamlandÄ±!")

# -------------------------------------------------------------------
# 13. Sistem DÃ¼zeltmeleri ve HazÄ±rlÄ±k
# -------------------------------------------------------------------

# --- 2. ADIM: SÄ°STEM DÃœZELTMELERÄ° VE HAZIRLIK ---

print("ğŸ› ï¸ Sistem kararlÄ±lÄ±ÄŸÄ± iÃ§in dÃ¼zeltmeler yapÄ±lÄ±yor...")

# 1. Cache TemizliÄŸi (OlasÄ± hatalarÄ± Ã¶nler)
cache_path = "/content/unsloth_compiled_cache"
if os.path.exists(cache_path):
    shutil.rmtree(cache_path)
    print("   -> Cache temizlendi.")

# 2. Psutil Global Fix (Unsloth iÃ§in gerekli)
builtins.psutil = psutil
print("   -> psutil global olarak ayarlandÄ±.")

# 3. Dosya KontrolÃ¼ (Opsiyonel bilgi)
RAW_DOC_PATH = "raw_document.md"
if not os.path.exists(RAW_DOC_PATH):
    print(f"âš ï¸ Bilgi: '{RAW_DOC_PATH}' dosyasÄ± ÅŸu an yok. (Sorun deÄŸil, demo iÃ§in kod iÃ§inde metin var.)")
else:
    print(f"âœ… '{RAW_DOC_PATH}' dosyasÄ± mevcut.")

print("âœ… Sistem eÄŸitime ve Ã§alÄ±ÅŸmaya hazÄ±r!")

# -------------------------------------------------------------------
# 14. MODEL KISMI (Tekrar - Yedek Kod BloÄŸu)
# -------------------------------------------------------------------

# --- 3. ADIM: MODEL EÄÄ°TÄ°MÄ° (FINE-TUNING) KODLARI ---
EGITIM_YAPILSIN_MI = False  # <--- Videoda burasÄ± False kalsÄ±n!

if EGITIM_YAPILSIN_MI:
    # AYARLAR
    HF_KULLANICI_ADI = "nilnilu"
    MODEL_ADI = "owasp-guvenlik-chatbot"
    QA_DATA_PATH = "qa_pairs.jsonl"

    # 1. Model YÃ¼kleme
    print("â³ EÄŸitim iÃ§in Temel Model YÃ¼kleniyor...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "unsloth/llama-3-8b-bnb-4bit",
        max_seq_length = 2048,
        dtype = None,
        load_in_4bit = True,
    )

    # LoRA AdaptÃ¶rleri
    model = FastLanguageModel.get_peft_model(
        model,
        r = 16,
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_alpha = 16,
        lora_dropout = 0,
        bias = "none",
        use_gradient_checkpointing = "unsloth",
        random_state = 3407,
    )

    # 2. Veri Seti HazÄ±rlÄ±ÄŸÄ±
    if os.path.exists(QA_DATA_PATH):
        print(f"ğŸ“š '{QA_DATA_PATH}' ile eÄŸitim baÅŸlÄ±yor...")
        alpaca_prompt = """### Instruction:
    {}

    ### Response:
    {}"""

        def formatting_prompts_func(examples):
            instructions = examples["instruction"]
            outputs      = examples["output"]
            texts = []
            for instruction, output in zip(instructions, outputs):
                text = alpaca_prompt.format(instruction, output) + tokenizer.eos_token
                texts.append(text)
            return { "text" : texts, }

        dataset = load_dataset("json", data_files=QA_DATA_PATH, split="train")
        dataset = dataset.map(formatting_prompts_func, batched = True,)

        # 3. EÄŸitim BaÅŸlatma
        trainer = SFTTrainer(
            model = model,
            tokenizer = tokenizer,
            train_dataset = dataset,
            dataset_text_field = "text",
            max_seq_length = 2048,
            dataset_num_proc = 2,
            packing = False,
            args = TrainingArguments(
                per_device_train_batch_size = 2,
                gradient_accumulation_steps = 4,
                warmup_steps = 5,
                max_steps = 60,
                learning_rate = 2e-4,
                fp16 = not torch.cuda.is_bf16_supported(),
                bf16 = torch.cuda.is_bf16_supported(),
                logging_steps = 1,
                optim = "adamw_8bit",
                weight_decay = 0.01,
                lr_scheduler_type = "linear",
                seed = 3407,
                output_dir = "outputs",
                report_to = "none",
            ),
        )
        trainer.train()

        # 4. Modeli Kaydetme
        print(f"ğŸš€ Model Hugging Face'e YÃ¼kleniyor: {HF_KULLANICI_ADI}/{MODEL_ADI}")
        # model.push_to_hub(f"{HF_KULLANICI_ADI}/{MODEL_ADI}", token=True) # Token env variable olmalÄ±
        # tokenizer.push_to_hub(f"{HF_KULLANICI_ADI}/{MODEL_ADI}", token=True)
        print("âœ… EÄÄ°TÄ°M TAMAMLANDI!")

    else:
        print(f"ğŸ›‘ HATA: '{QA_DATA_PATH}' dosyasÄ± bulunamadÄ±!")
else:
    print("â„¹ï¸ EÄÄ°TÄ°M ADIMI ATLANDI.")


# -------------------------------------------------------------------
# 15. STREAMLIT ArayÃ¼zÃ¼
# -------------------------------------------------------------------

# 1. TEMÄ°ZLÄ°K VE KURULUMLAR
print("ğŸ§¹ Sistem Temizleniyor...")
os.system("fuser -k 8000/tcp")
os.system("fuser -k 8501/tcp")
gc.collect()
torch.cuda.empty_cache()

# Gerekli kÃ¼tÃ¼phaneler
# !pip install -q streamlit pyngrok uvicorn fastapi unsloth langchain-community chromadb sentence-transformers

# Token giriÅŸi (EÄŸer kayÄ±tlÄ±ysa otomatiktir, deÄŸilse manuel girilir)
try:
    print("ğŸ”‘ Hugging Face KontrolÃ¼...")
    # Token'Ä± buraya string olarak da yazabilirsin: login(token="hf_...")
    login(token=os.getenv("HF_TOKEN"))
except: pass

# --- AYARLAR ---
HF_KULLANICI_ADI = "nilnilu"
MODEL_ADI = "owasp-guvenlik-chatbot"
DOC_NAME = "raw_document.md"

# --- 2. DÃœZELTÄ°LMÄ°Å BÄ°LGÄ° BANKASI (DOÄRU CEVAPLAR) ---
# Buradaki cevaplar sunumda jÃ¼riyi etkileyecek teknik doÄŸruluktadÄ±r.
FIXED_DOCUMENT = """
[GÃœVENLÄ°K BÄ°LGÄ° BANKASI]

>>> SORU: LOGLAR KAÃ‡ GÃœN SAKLANIR?
CEVAP: Yasal zorunluluklar (5651 SayÄ±lÄ± Kanun) ve kurum politikalarÄ± gereÄŸi loglar gÃ¼venli ortamda 90 gÃ¼n boyunca saklanmaktadÄ±r.

>>> SORU: PAROLA HASHLEME GÃœVENLÄ°ÄÄ° NEDÄ°R?
CEVAP: Parolalar saklanÄ±rken "Tuzlama" (Salting) yÃ¶ntemi zorunludur. Bu iÅŸlem, Rainbow Table saldÄ±rÄ±larÄ±na karÅŸÄ± koruma saÄŸlar.

>>> SORU: A08:2021 TANIMI NEDÄ°R?
CEVAP: "YazÄ±lÄ±m ve Veri BÃ¼tÃ¼nlÃ¼ÄŸÃ¼ HatalarÄ±"; yazÄ±lÄ±m gÃ¼ncellemeleri, CI/CD sÃ¼reÃ§leri ve veri doÄŸrulama mekanizmalarÄ±ndaki eksiklikleri kapsar.

>>> SORU: ZERO DAY (SIFIRINCI GÃœN) NEDÄ°R?
CEVAP: YazÄ±lÄ±m Ã¼reticisinin henÃ¼z haberdar olmadÄ±ÄŸÄ± ve yamasÄ± (patch) yayÄ±nlanmamÄ±ÅŸ gÃ¼venlik zafiyetlerine verilen genel isimdir. (Not: Kurum iÃ§i zafiyet durumu GÄ°ZLÄ°DÄ°R).

>>> SORU: KRÄ°TÄ°K ZAFÄ°YET SÃœRECÄ° NASIL Ä°ÅLER?
CEVAP: Kritik bulgu tespit edildiÄŸinde Ã¼retim sÃ¼reci durdurulur ve 24 saat iÃ§inde acil yama (hotfix) geÃ§ilmesi zorunludur.

>>> SORU: RÄ°SK DÃœZELTME SÃœRELERÄ° NEDÄ°R?
CEVAP: YÃ¼ksek (High) riskli bulgular 3 iÅŸ gÃ¼nÃ¼, Orta (Medium) riskli bulgular 7 iÅŸ gÃ¼nÃ¼ iÃ§inde kapatÄ±lmalÄ±dÄ±r.
>>> SORU: ÅÄ°FRE POLÄ°TÄ°KASI NEDÄ°R?
CEVAP: En az 12 karakter uzunluÄŸunda olmalÄ±; bÃ¼yÃ¼k harf, kÃ¼Ã§Ã¼k harf, rakam ve Ã¶zel karakter iÃ§ermelidir.

>>> SORU: SQL ENJEKSÄ°YONU NASIL Ã–NLENÄ°R?
CEVAP: Dinamik SQL kullanÄ±mÄ± yasaktÄ±r. Mutlaka "Parametreli Sorgular" (Prepared Statements) veya ORM kullanÄ±lmalÄ±dÄ±r.

>>> SORU: SUNUCU MARKASI NEDÄ°R?
CEVAP: AltyapÄ± ve donanÄ±m envanter bilgisi, gÃ¼venlik politikasÄ± gereÄŸi GÄ°ZLÄ°DÄ°R ve paylaÅŸÄ±lamaz.

>>> SORU: CI/CD HATTINA ERÄ°ÅÄ°M NASIL OLMALI?
CEVAP: CI/CD hattÄ±na sadece yetkilendirilmiÅŸ kullanÄ±cÄ±lar, MFA (Ã‡ok FaktÃ¶rlÃ¼ Kimlik DoÄŸrulama) ile eriÅŸmelidir.
"""

with open(DOC_NAME, "w", encoding="utf-8") as f:
    f.write(FIXED_DOCUMENT)

# --- 3. BACKEND (FASTAPI) ---
# app = FastAPI() # YukarÄ±da tanÄ±mlanmÄ±ÅŸtÄ±, tekrar tanÄ±mlamaya gerek yok ama baÄŸlam iÃ§in burada.
model = None
tokenizer = None

# PROMPT AYARI: Modele kesin sÄ±nÄ±rlar Ã§iziyoruz
RAG_PROMPT_SYSTEM = """Sen uzman bir Siber GÃ¼venlik AsistanÄ±sÄ±n.
GÃ–REV: AÅŸaÄŸÄ±daki [BÄ°LGÄ° BANKASI] metnini kullanarak soruyu cevapla.

KURALLAR:
1. Sadece verilen metindeki bilgiyi kullan.
2. EÄŸer sorunun cevabÄ± "GÄ°ZLÄ°DÄ°R" iÃ§eriyorsa, bunu aÃ§Ä±kÃ§a belirt ve reddet.
3. KÄ±sa, net ve profesyonel cevap ver.
[BÄ°LGÄ° BANKASI]:
{context}

KullanÄ±cÄ± Sorusu: {question}
Cevap:"""

def setup_system():
    global model, tokenizer
    print(f"\nğŸš€ [Sistem] Model YÃ¼kleniyor: {HF_KULLANICI_ADI}/{MODEL_ADI}")
    try:
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name = f"{HF_KULLANICI_ADI}/{MODEL_ADI}",
            max_seq_length = 2048,
            dtype = None,
            load_in_4bit = True,
        )
        FastLanguageModel.for_inference(model)
        print("âœ… Model BaÅŸarÄ±yla YÃ¼klendi.")
    except Exception as e:
        print(f"ğŸ›‘ Model HatasÄ±: {e}")

# setup_system() # Hata almamak iÃ§in manuel Ã§aÄŸÄ±rÄ±lmalÄ±

@app.post("/chat_stream")
async def chat_endpoint_stream(request_data: dict):
    global model, tokenizer
    question = request_data.get("question")
    if not model: return {"response": "Model henÃ¼z yÃ¼klenmedi, lÃ¼tfen bekleyin..."}

    # Promptu hazÄ±rla
    prompt = RAG_PROMPT_SYSTEM.format(context=FIXED_DOCUMENT, question=question)

    # Tokenizer ayarlarÄ±
    inputs = tokenizer([prompt], return_tensors="pt", padding=True).to("cuda")

    # Ãœretim ayarlarÄ± (Deterministik olmasÄ± iÃ§in temperature dÃ¼ÅŸÃ¼k)
    outputs = model.generate(
        **inputs,
        max_new_tokens=128,
        use_cache=True,
        pad_token_id=tokenizer.eos_token_id,
        temperature=0.01,  # Daha kararlÄ± cevaplar iÃ§in dÃ¼ÅŸÃ¼rdÃ¼k
        do_sample=True
    )

    decoded = tokenizer.batch_decode(outputs)[0]

    # CevabÄ± ayrÄ±ÅŸtÄ±r (Parsing)
    if "Cevap:" in decoded:
        response = decoded.split("Cevap:")[-1].strip().replace(tokenizer.eos_token, "")
    else:
        # EÄŸer model prompt formatÄ±nÄ± bozarsa ham Ã§Ä±ktÄ±yÄ± temizle
        response = decoded.split("KullanÄ±cÄ± Sorusu:")[-1].strip()

    return {"response": response}

def run_api():
    uvicorn.run(app, host="127.0.0.1", port=8000)

# thread = threading.Thread(target=run_api)
# thread.start()

# --- 4. FRONTEND (STREAMLIT) ---
streamlit_code = """
import streamlit as st
import requests
import time

# Sayfa AyarlarÄ±
st.set_page_config(page_title="CyberSec AI", page_icon="ğŸ›¡ï¸", layout="centered")

# BaÅŸlÄ±k TasarÄ±mÄ±
st.markdown("<h1 style='text-align: center; color: #00FF41;'>ğŸ›¡ï¸ Siber GÃ¼venlik AsistanÄ±</h1>", unsafe_allow_html=True)
st.markdown("<h4 style='text-align: center; color: gray;'>RAG Destekli Kurumsal GÃ¼venlik Botu</h4>", unsafe_allow_html=True)
st.divider()

# Session State (GeÃ§miÅŸi tutmak iÃ§in)
if "messages" not in st.session_state:
    st.session_state.messages = []

# GeÃ§miÅŸ mesajlarÄ± ekrana bas
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# KullanÄ±cÄ± giriÅŸi
if prompt := st.chat_input("GÃ¼venlik prosedÃ¼rleri hakkÄ±nda bir soru sorun..."):
    # KullanÄ±cÄ± mesajÄ±nÄ± ekle
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Bot cevabÄ±nÄ± al
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        with st.spinner("Veri tabanÄ± taranÄ±yor ve cevap Ã¼retiliyor..."):
            try:
                res = requests.post("http://127.0.0.1:8000/chat", json={"question": prompt})
                if res.status_code == 200:
                    full_response = res.json().get("response", "Cevap alÄ±namadÄ±.")

                    # Daktilo efekti (GÃ¶rsellik iÃ§in)
                    displayed_response = ""
                    for char in full_response:
                        displayed_response += char
                        message_placeholder.markdown(displayed_response + "â–Œ")
                        time.sleep(0.01)
                    message_placeholder.markdown(displayed_response)

                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                else:
                    st.error("API BaÄŸlantÄ± HatasÄ±!")
            except Exception as e:
                st.error(f"Hata oluÅŸtu: {e}")
"""
with open("app.py", "w", encoding="utf-8") as f:
    f.write(streamlit_code)

# --- 5. BAÅLATMA VE TÃœNEL ---
print("â³ Streamlit BaÅŸlatÄ±lÄ±yor...")
if os.path.exists("streamlit.log"): os.remove("streamlit.log")
log_file = open("streamlit.log", "w")

# Streamlit'i arka planda Ã§alÄ±ÅŸtÄ±r
# subprocess.Popen([sys.executable, "-m", "streamlit", "run", "app.py", "--server.port", "8501", "--server.address", "127.0.0.1"], stdout=log_file, stderr=log_file)

time.sleep(5)

print("\nğŸŒ AÅAÄIDAKÄ° LÄ°NKE TIKLAYARAK ARAYÃœZE GÄ°DEBÄ°LÄ°RSÄ°NÄ°Z:")
if not os.path.exists("cloudflared"):
    # !wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64
    # !mv cloudflared-linux-amd64 cloudflared
    # !chmod +x cloudflared
    pass

# Cloudflare tÃ¼neli
# !./cloudflared tunnel --url http://127.0.0.1:8501
